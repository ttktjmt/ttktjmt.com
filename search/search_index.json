{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#hi-i-am-tatsuki-tsujimoto","title":"Hi \ud83d\udc4b\u00a0 I am Tatsuki Tsujimoto","text":"<p> As a researcher and software developer, my driving purpose is to apply scientific knowledge and technological advancement to create meaningful solutions for people with difficulties. My key areas of interest include biomedical engineering, neuroscience, healthcare, wearable technology, and artificial intelligence. </p> <p></p> <p>Portfolio Website: https://ttktjmt.com Source Code: https://github.com/ttktjmt/ttktjmt.com</p>"},{"location":"#more-about-me","title":"More About Me","text":"<ul> <li>        \u00a0 I currently live in Kanagawa, Japan</li> <li>     \u00a0 I speak Japanese, English, and some German</li> <li>        \u00a0 I enjoy playing basketball and watching NBA games</li> <li>             \u00a0 My favorite book at the moment is \"The Three-Body Problem\"</li> <li>              \u00a0 Besides the above academic fields, I'm also interested in robotics, linguistics, and philosophy</li> <li>  \u00a0 You can find my contact info in the Contact Tab</li> </ul>"},{"location":"#this-website-is-running-on-my-raspberry-pi-4","title":"This Website is running on my Raspberry Pi 4","text":"<p>The web server for this site is running in the kubernetes cluster on my Raspberry Pi 4 . You can check out the About Tab for more configuration details of this website.</p> <p></p>"},{"location":"contact/","title":"Contact","text":"<p>I am always excited to connect with others regarding potential job opportunities where I can contribute my skills and expertise. If you have a project or a role in mind that aligns with my background, please don't hesitate to reach out.</p>"},{"location":"contact/#im-open-to-messages-about","title":"I'm Open to Messages About:","text":"<ul> <li>Job Offer: Full-time positions, freelance opportunities, or temporary projects.</li> <li>Collaboration: Joining forces on projects that can benefit from my skill set.</li> <li>Networking: Connecting with professionals in my field or related fields for mutual growth.</li> </ul>"},{"location":"contact/#please-refrain-from-sending","title":"Please Refrain From Sending:","text":"<ul> <li>Advertisement: I am not interested in unsolicited sales pitches or promotional messages.</li> <li>Personal Request: Non-professional requests or inquiries unrelated to my work and expertise.</li> </ul>"},{"location":"contact/#contact-info","title":"Contact Info","text":"<p>You are welcome to connect with me on Linkedin.</p> <p>   You can send me an email to </p> <pre><code>contact [at] ttktjmt {dot} com\n</code></pre> <p>Email from Default Email App </p>"},{"location":"experience/","title":"Experience     Software Developer      Researcher / Software Developer      Researcher / Data Scientist / MVP Developer","text":"Computomics \u00b7 Internship / Full-time \u00b7 7 mos / now <p> Contributed to the development of a tool that visualizes genomic data from agricultural sources and identifies genome patterns linked to desired traits (e.g., disease resistance, fruit yield). Designed and implemented a C++ backend CLI tool to convert large genomic datasets for the tool as an extension of an Open Source Software. Independently built a Python FastAPI to seamlessly deliver graph data to the front-end. Orchestrated the cloud deployment of the entire system using Docker, Kubernetes, and AWS services, enabling online operation and scalability. Actively learned genomic analysis, networking concepts, and cloud technologies, demonstrating strong problem-solving abilities and technical growth. </p> Skills <p>             C++ ,                 Python ,                FastAPI ,             Kubernetes ,        AWS (IAM, EKS, S3, RDS) ,             PostgreSQL ,                 GitLab ,                Rancher ,                   Lens ,   CLI Tool Dev ,                Machine Learning ,      Software Development</p> <p></p> <p></p> Medita \u00b7 Internship \u00b7 1 yr 9 mos <p> As a member of R&amp;D team, I focused on healthcare applications by analyzing navel temperature for IoT integration. Contributed to developing a mobile application and IoT devices, and collaborated with external companies on experimental design and execution. </p> Skills <p>            Python ,            Flutter ,               Dart ,           Firebase ,             GitHub ,          Mobile App Dev ,          BLE Dev ,         Database ,        Data Analysis ,  Software Development</p> <p></p> <p></p> Sci-bone \u00b7 Self-employed \u00b7 8 mos <p> Co-founded a wearable device company with a colleague, and actively participated in various business competitions. Also represented Nagano Prefecture at the Entrepreneur Expo, hosted by the Ministry of Internal Affairs and Communications and the NICT </p> Skills <p>            Python ,            Arduino ,        Data Analysis ,  Software Development</p> <p></p>"},{"location":"projects/","title":"Personal Projects","text":""},{"location":"projects/#mjswan","title":"mjswan","text":"<p>mjswan is a browser-based mujoco playground built on top of mujoco wasm, onnxruntime, and three.js. This enables MuJoCo simulations with real-time trained policy control, running entirely in the browser - no server for simulation required. Working closely with the mujoco team at Google DeepMind, mjlab and viser developers at UC Berkeley, a gaming company, etc.</p> Skills <p>        MuJoCo ,            ONNX Runtime ,        Vue.js ,         Vuetify ,            Vite ,       Node.js ,             NPM ,      Three.js ,  MyoSuite ,          Python ,           HTML ,             CSS ,      JavaScript ,     WebAssembly ,          GitHub ,         Reinforcement Learning</p>"},{"location":"projects/#mjlab_myosuite","title":"mjlab_myosuite","text":"<p>Reimplemented the MyoChallenge 2022 \"Die Reorientation\" Task in mjlab environment.</p> Skills <p>          Mjlab ,            Viser ,  MyoSuite ,  Weights &amp; Biases ,           MuJoCo ,            Python ,            GitHub ,           Reinforcement Learning</p>"},{"location":"projects/#myochallenge-2024","title":"MyoChallenge 2024","text":"<p>As a member of the MyoSuite community and the MyoChallenge Advocacy Team, I created a blog explaining the general process of participating in the MyoChallenge 2024 competition. The blog was cited on the official website and featured by the official account on SNS (Linkedin, X) Formed a team neuroflex with international enthusiasts and won 3rd place in the Bimanual Task and the DEI Team Award.</p> Skills <p> MyoSuite ,       Gym ,         MuJoCo ,         PyTorch ,          Docker ,          Python ,          GitHub ,         Reinforcement Learning</p>"},{"location":"projects/#portfolio-website","title":"Portfolio Website","text":"<p>Check out the About tab for detailed information.</p> Skills <p>  Material for Mkdocs ,          Kubernetes ,                 k3s ,              Docker ,              Python ,               HTML , :simple-css3:               CSS ,          JavaScript ,              GitHub ,       GitHub Actions ,          Cloudflare ,         Raspberry Pi</p>"},{"location":"projects/#galton-board-simulator","title":"Galton Board Simulator","text":"<p>The Galton Board, also known as the Galton box, quincunx or bean machine, is a device that visually demonstrates the central limit theorem by dropping balls through a pegged board, resulting in a normal distribution. I developed a 2D simulator as an Android app using box2d as a physics engine. It uses the accelerometer sensor in the device (phone, laptop) to change the gravity in the simulation, resulting in a different distribution.</p> Skills <p>      C++ ,              Qt ,         Box2D (Physics Engine) ,          GitHub</p>"},{"location":"projects/#overall-manufacturing-process-of-myoelectric-prosthetic-hand","title":"Overall Manufacturing Process of Myoelectric Prosthetic Hand","text":"<p>Prior to joining the lab, I had the opportunity to engage in the overall process of creating a myoelectric prosthetic hand. The process I experienced involved the following key components:</p> <ul> <li>Manufacturing a Conductive Silicone sEMG Sensor and a Sensor Band to wrap them around the forearm</li> <li>Building an embedded system, which included:<ul> <li>Assembling the hardware components</li> <li>Writing the firmware (Signal Processing, Feature Extraction, Neural Network, Bluetooth Communication, etc) in C</li> </ul> </li> <li>Constructing the mechanical hand itself</li> <li>Developing a tablet application in C++ to interface with the prosthetic hand</li> <li>Silicone Finger Manufacturing (as part of another student's research)</li> </ul> Skills <p>              C (programming language) ,       C++ ,              Qt </p>"},{"location":"projects/#voice-controlled-robot-hand","title":"Voice Controlled Robot Hand","text":"<p>I built my own robot hand from cardboard, servo motors, string, etc., connected it to a Raspberry Pi and a USB microphone, and used voice recognition to control it with my voice. The idea behind this project was to enable anyone to operate a high DOF prosthetic hand without training, as I felt that operating a prosthetic hand with electrical signals from muscles is intuitive but difficult to achieve high DOF.</p> Skills <p>         Python ,     Raspberry Pi</p>"},{"location":"projects/#tetris-on-arduino-game-console","title":"TETRIS on Arduino Game Console","text":"<p>I created a classic game console using an Arduino Nano and a lot of LEDs. To extend the controllable pin capacity of the Arduino Nano, I used shift registers (SN74HC595). I soldered 136 LEDs by hand onto a universal board (I know, it's a menace...), allowing independent control of each LED via the Arduino Nano. Using matrix processing, I developed TETRIS in the Arduino language, carefully implementing boundary conditions and block collision avoidance to ensure that the game runs smoothly and is less prone to bugs.</p> Skills <p>            Arduino ,    Soldering ,   Game Dev</p>"},{"location":"projects/#robot-battle-competition","title":"Robot Battle Competition","text":"<p>As part of a class project, our team developed two robots for the Robot Battle Competition. We had to adhere to strict criteria, (e.g., weight, size, cost limitations), to develop our robots that can remain on stage for a longer period of time than competitors. As a result of our efforts, we won 2nd place in the competition and also received the Best Design Award.</p> Skills <p>                        Arduino ,                Soldering ,                    Signal Processing ,                         Robotics ,    Electronics</p>"},{"location":"about/cicd/","title":"CI/CD","text":"<p>This project leverages GitHub Actions to automate the deployment to GitHub Pages and the Docker image. By implementing the following two workflows, this project benefits from a fully automated CI/CD pipeline, ensuring that the documentation site and the Docker image are always up-to-date and accessible to users.</p>"},{"location":"about/cicd/#deployment-to-github-pages","title":"Deployment to GitHub Pages","text":"<pre><code>graph TB\n    A[Push to dev/main branch] --&gt;|Triggers| B\n\n    subgraph B[GitHub Actions: gh-pages.yml]\n        direction LR\n        C[Prepare Build&lt;br&gt;Environment] --&gt; D[Build Documentation]\n        D --&gt; F[Update GitHub Pages&lt;br&gt;ttktjmt.github.io/ttktjmt.com]\n    end</code></pre> <p>The deployment of MkDocs to GitHub Pages is automated through the <code>gh-pages.yml</code> workflow, which is triggered by <code>push</code> events to the <code>dev</code> and <code>main</code> branches. This ensures that any changes aimed at testing design, functionality, and more are automatically reflected on the live documentation site. The documentation on GitHub Pages will be updated whenever commits are made, even if it's still under development. It utilizes the <code>mkdocs gh-deploy</code> command for immediate publication.</p> Info <p>The website you see from the <code>ttktjmt.com</code> address can differ from the one at the <code>ttktjmt.github.io/ttktjmt.com</code> address. The <code>ttktjmt.com</code> site represents the public version of the documentation, aiming to be shown to other people. On the other hand, the <code>ttktjmt.github.io/ttktjmt.com</code> site represents the site under development.</p>"},{"location":"about/cicd/#docker-image-deployment","title":"Docker Image Deployment","text":"<pre><code>graph TB\n    A[Publish Release] --&gt;|Triggers| B\n    A --&gt;|Version Tag| I\n\n    subgraph S[GitHub Secrets]\n        S1[Docker Hub Username]\n        S2[Docker Hub Password]\n    end\n\n    subgraph B[GitHub Actions: docker.yml]\n        direction LR\n        C[Prepare Build Environment] --&gt; D[Login to Docker Hub]\n        S --&gt; D\n        D --&gt; I[Build and Push Docker Image]\n\n    end\n\n    subgraph E[Docker Hub]\n        subgraph J[Docker Image]\n            direction LR\n            F[Version-specific Tag: v0.1.2]\n            G[Tag: latest]\n        end\n    end\n\n    I --&gt;|Pushes| J\n\n    H[k3s Cluster] --&gt;|Pulls| G</code></pre> <p>The process of building and pushing a Docker image to Docker Hub is automated through the <code>docker.yml</code> workflow, which is triggered by <code>published</code> events for GitHub Releases. This ensures that each new release on the repo creates the latest version of the Docker image on Docker Hub.</p> <p>This workflow extracts the version tag from the Git tag that triggered the release event and uses it to tag the Docker image. Also, it updates the image with the <code>latest</code> tag simultaneously so that the deployment on the k3s cluster can use the latest image easily, also ensuring that the site runs on the latest version.</p> Info <p>Credential info (username and password) for Docker Hub is stored as a secret in the GitHub repo. The workflow uses this data to run the <code>docker login</code> command.</p>"},{"location":"about/overview/","title":"Overview","text":"<p>Info</p> <p>The following architecture has been temporarily suspended due to security concerns.</p> <p>This website is developed for showcasing my information, managing my projects, hosting and sharing my blog posts, and etc. The web server for this site is running as a deployment resource in the k3s cluster on my raspberry pi 4 at home. The service is exposed by utilizing Traefik and Cloudflare Tunnel.</p>"},{"location":"about/overview/#architecture","title":"Architecture","text":"Skills <p>  Kubernetes ,            k3s ,         Docker ,         Python ,         GitHub ,          Linux ,    Raspberry Pi</p>"},{"location":"about/versioning/","title":"Versioning","text":"<p>This website adheres to the principles of Semantic Versioning (SemVer), a widely adopted versioning scheme in the software development industry. The current version of this site can be found on the top right corner of any page.</p>"},{"location":"about/versioning/#what-is-semantic-versioning","title":"What is Semantic Versioning?","text":"<p>Semantic Versioning is a versioning system that conveys meaningful information about the nature of version changes and compatibility between versions. A version number consists of three parts: MAJOR, MINOR, and PATCH, formatted as <code>MAJOR.MINOR.PATCH</code>.</p> Version Type Description <code>MAJOR</code> Incompatible API changes <code>MINOR</code> New backward-compatible functionality <code>PATCH</code> Backward-compatible bug fixes"},{"location":"about/versioning/#why-semantic-versioning","title":"Why Semantic Versioning?","text":"<p>Semantic Versioning provides a clear and consistent way to communicate changes in software versions, ensuring that users and developers can understand the implications of updating to a new version. It promotes better collaboration, easier maintenance, and improved compatibility across different components and systems.</p>"},{"location":"about/versioning/#versioning-of-this-website","title":"Versioning of This Website","text":"<p>The versioning of this website is managed through a Git workflow involving the <code>main</code> and <code>dev</code> branches. New features and improvements are developed and tested under the <code>dev</code> branch. Once a set of changes is thoroughly reviewed and deemed stable, they are merged into the <code>main</code> branch, and a new version will be released by following the specific guidelines below (based on the SemVer scheme).</p> MAJORMINORPATCH <ul> <li>Structural changes (e.g. changing the framework, migrating to a new static site generator)</li> <li>Significant design changes</li> <li>Changes that break compatibility with existing themes or plugins</li> <li>Substantial reorganization of the website's content structure or navigation</li> </ul> <ul> <li>Page changes (e.g. adding/deleting new pages)</li> <li>New features or sections to the website</li> <li>New plugins or themes</li> <li>Upgrading dependencies (e.g. MkDocs, Python, etc.)</li> <li>Improving accessibility or performance</li> </ul> <ul> <li>Content changes (e.g. updating existing content, fixing broken links)</li> <li>Typo fixes</li> <li>Fixing bugs or issues</li> <li>Updating dependencies for security or bug fixes</li> <li>Minor styling or CSS adjustments</li> </ul> <p>This approach ensures that the <code>main</code> branch always represents the latest stable release, while the <code>dev</code> branch serves as a staging area for upcoming changes. By adhering to this process, I can maintain a robust and reliable website while continuously improving and adding new functionality.</p> <p>Note</p> <p>The version tag <code>v0.0.0</code> is used for general purpose of testing (such as checking if the GitHub Actions workflow works properly)</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/discovering-the-best-way-to-acquire-a-new-language/","title":"Discovering the Best Way to Acquire a New Language","text":"<p>Last year, I participated in a Japanese national scholarship program called Vulcanus in Europe, which involved spending six months studying German at a language school in Germany, followed by a six-month full-time internship at a German company.</p> <p>In this blog, I will share the insights I gained from learning German throughout the year and the discoveries I made about effective language acquisition.</p>"},{"location":"blog/discovering-the-best-way-to-acquire-a-new-language/#initial-learning-experience","title":"Initial Learning Experience","text":"<p>When I started learning German, I had no prior knowledge (A1 level). After six months of studying at a language school, I reached a level where I could barely manage basic conversations (B1 level). Even after the language school period ended, I continued studying German daily using language learning apps like Duolingo and Busuu.</p> <p>However, around nine months into my German studies, I realized that my progress was not matching the effort I was putting in. I felt a growing sense of unease about whether I was on the right track. Knowing that some people advanced from A1 to C1 within six months at language school made me question the gap between their progress and mine.</p> <p>Believing that there were many other people who might share my frustrations, I decided to investigate scientific research on language learning. The internet was flooded with anecdotal advice, often with high price tags, so I focused on scholarly articles and reputable academics. This led me to Stephen Krashen's Input Hypothesis.</p>"},{"location":"blog/discovering-the-best-way-to-acquire-a-new-language/#discovering-the-input-hypothesis","title":"Discovering the Input Hypothesis","text":"<p>The Input Hypothesis, in a nutshell, suggests that the most effective way to acquire a second language is through receiving large amounts of comprehensible input, which means language that is slightly above the learner's current level (i+1). Key points from this hypothesis include:</p> <ul> <li>The need for comprehensible input</li> <li>The importance of sustained exposure over time</li> <li>The limited role of output (speaking/writing), especially in early stages</li> </ul> <p>Krashen argues that these principles are fundamental for language acquisition. It's naive to say \"Input Is All You Need\", but it's worth mentioning that input plays a key role in language acquisition.</p>"},{"location":"blog/discovering-the-best-way-to-acquire-a-new-language/#personal-insights-on-language-learning","title":"Personal Insights on Language Learning","text":"<p>On discovering this hypothesis, I finally understood how I had learned English. Although I still have many things to learn about English, I was often asked how I became proficient in it, and I struggled to give a clear answer. My strong motivation to learn English from middle school led me to various activities - watching English gaming videos on YouTube, participating in exchange programs in the US, and practicing with international students in Japan. I thought each method contributed to my English skills, but I couldn't pinpoint which method was most effective.</p> <p>Krashen\u2019s theory clarified my experience: I had been continuously receiving comprehensible input over a long period while enjoying the process.</p>"},{"location":"blog/discovering-the-best-way-to-acquire-a-new-language/#changing-my-strategy","title":"Changing My Strategy","text":"<p>Inspired by this realization, I significantly changed my approach to learning German. I shifted from \"studying\" to genuinely \"acquiring\" the language through enjoyable and effective methods. Specifically, I uninstalled Duolingo and Busuu and focused on the following activities:</p> <ol> <li> <p>Watching Studio Ghibli movies in German</p> <p>Re-watching these films helped me get extended periods of comprehensible input. Being familiar with the storylines made it easier to infer the meanings of unknown words, which greatly expanded my vocabulary.</p> </li> <li> <p>Watching short videos on TikTok in German</p> <p>This method is great for those who struggle with motivation. Short videos allow you to experience how native speakers communicate and learn about their culture. TikTok's algorithm also helps by showing content that matches your interests, turning the addictive nature of short videos into a learning advantage. Setting the app to German content is particularly effective.</p> </li> <li> <p>Listening to German podcasts</p> <p>Once you reach a certain level of comprehension, podcasts become a powerful source of input. Unlike other methods, you cannot turn on subtitles, so some understanding is necessary. Finding podcasts on topics you genuinely enjoy allows you to make use of downtime, like doing chores or commuting. My recommendation is to listen to content that interests you rather than language learning podcasts.</p> </li> </ol>"},{"location":"blog/discovering-the-best-way-to-acquire-a-new-language/#reflections","title":"Reflections","text":"<p>Implementing these methods and significantly increasing the time spent on them led to noticeable improvements in my German, especially in comprehension and speaking skills. By the end of my stay in Germany, I was able to converse almost entirely in German at my farewell party with my co-workers.</p> <p>Although I\u2019m not yet confident enough to say that \u201cI am fluent in German,\"  I am now assured that with the right approach and sufficient time, I can achieve it. Moreover, I\u2019ve discovered that acquiring a new language can be more enjoyable and exciting than commonly believed, motivating me to continue exploring new languages. And it\u2019s free!!</p> <p>Finally, it\u2019s important to note, however, that while Krashen\u2019s Input Hypothesis is influential and widely regarded, it is not universally accepted as the definitive method for language acquisition. The field of language learning is still evolving, and ongoing research continues to refine our understanding of how best to learn new languages. If you are interested, I also recommend you fall into the rabbit hole with me!!</p> <p></p> <p>Thank you for reading my blog. I hope it serves as a helpful guide in your language learning journey.</p> <p>Auf Wiedersehen  </p>"},{"location":"blog/real-time-scatter3d-embedded-in-qwidget-based-application/","title":"Real-time Scatter3D Embedded in QWidget-based Application","text":"<p>This article is to summarize my recent research on how to use the Scatter3D QML graph in the Qt project. There were not sufficient documents and examples that explained how you can use a Qt Quick component in a QWidget-based application, and also how you can manipulate objects in the graph from C++ code, as far as I searched.</p> <p></p> <p>The reason why I use the Scatter3D QML over the Q3DScatter class while Q3DScatter has better integration with the QWidget application is that the Scatter3D supports features like legends out-of-the-box, which are more difficult to implement with Q3DScatter, as described in this forum entry. Qt Forum - Copying C++ Object Data to QML Object (Example: Q3DScatter to Scatter3D)</p> <p>I created a GitHub repository for this demo named scatter3d-in-qwidget, so you can check all of the source code from there. GitHub - ttktjmt/scatter3d-in-qwidget</p>"},{"location":"blog/real-time-scatter3d-embedded-in-qwidget-based-application/#supported-rendering-apis-for-qt-quick-3d","title":"Supported Rendering APIs for Qt Quick 3D","text":"<p>According to Qt Quick 3D documentation, Qt Quick 3D requires at least one of the following APIs: </p> <ul> <li>OpenGL 3.3+ (support from 3.0)</li> <li>OpenGL ES 3.0+ (limited support for OpenGL ES 2)</li> <li>Direct3D 11.1</li> <li>Vulcan 1.0+</li> <li>Metal 1.2+</li> </ul> <p>It\u2019s worth mentioning that you should check your device's OpenGL version when you have some trouble with 3D objects in the graph.</p>"},{"location":"blog/real-time-scatter3d-embedded-in-qwidget-based-application/#qquickview-vs-qquickwidget","title":"<code>QQuickView</code> v.s. <code>QQuickWidget</code>","text":"<p>There are currently two ways to use Qt Quick components in a QWidget-based project:\u00a0<code>QQuickView</code> with <code>QWidget::createWindowContainer()</code>,\u00a0and\u00a0<code>QQuickWidget</code>. Of course there are pros and cons, but when you want to embed the graph partially in an existing Qt Widgets application, you would need to use QQuickWidget since the QQuickView component is not resizable and only visible in a fixed rectangular area of a display.</p> <p>So, to summarize the usage of both classes, </p> <ul> <li><code>QQuickView</code> : high-performance, but not resizable</li> <li><code>QQuickWidget</code> : flexible, at the expense of performance</li> </ul> <p>In this article, I use QQuickWidget, assuming that you need to integrate Qt Quick Content partially into the existing large-scale QWidget-based project.</p>"},{"location":"blog/real-time-scatter3d-embedded-in-qwidget-based-application/#set-up-3d-scatter-graph","title":"Set up 3D Scatter Graph","text":"<ul> <li> <p>Make sure that the following components were already added from the Qt Maintenance Tool:</p> </li> <li> <p>Qt Quick 3D</p> </li> <li>Additional Libraries / Qt 3D</li> <li>Additional Libraries / Qt Data Visualization</li> </ul> <p>After creating a Qt Widgets Application on Qt Creator, I added several random components with <code>QWidget</code> (this will be promoted to the custom class I will declare later on) in the same <code>scrollAreaWidget</code> to test if the scatter graph was correctly set partially in the qt widget application.</p>"},{"location":"blog/real-time-scatter3d-embedded-in-qwidget-based-application/#create-a-wrapper-class-of-scatter3d-in-c","title":"Create a Wrapper Class of Scatter3D in C++","text":"<p>Add a new C++ Class (h/cpp) with the base class as QWidget (I named this class as <code>MyScatter3D</code> in the example project). From the design tab, you can promote the QWidget component to the MyScatter3D class by adding the class on the \u201cPromoted Widgets\u201d dialog.</p>"},{"location":"blog/real-time-scatter3d-embedded-in-qwidget-based-application/#declare-the-scatter3d-with-qml-files","title":"Declare the Scatter3D with QML files","text":"<p>Declare the Scatter3D graph with a new QML file named <code>graph.qml</code>, and source the qml file to a new QQuickWidget instance declared in the wrapper class constructor.</p> <p>You can enable touch gestures on Android devices with the <code>setAttribute(Qt::WA_AcceptTouchEvents)</code> statement.</p>"},{"location":"blog/real-time-scatter3d-embedded-in-qwidget-based-application/#interacting-with-qml-objects-from-c-file","title":"Interacting with QML Objects from C++ file","text":"<p>You can get pointers to the QML objects declared in the QML file with <code>findChild</code> method.</p> <pre><code>dataView = quickWidget-&gt;rootObject()-&gt;findChild&lt;QQuickItem*&gt;(\"dataView\");\n</code></pre> <p>It\u2019s important to know that this method searches for an object with its name, not an ID. So, don't forget to add an <code>objectName</code> attribute to the target object.</p> <p>I added three public slots; <code>AddPlot</code>, <code>RemovePlot</code>, <code>RealTimePlot</code> in the MyScatter3D class, and connected them with two buttons and a timer instance respectively. All functions declared in the C++ file are calling functions declared in the QML file with <code>QMetaObject::invokeMethod(object, \"function_name\")</code> method. You can now add/remove a plot from buttons, and also see the real-time pointer moving randomly in the graph.</p> <p>Somehow you need to find the object every time you call the function, and it cannot be stored as a member variable. There might be some ways to solve this problem.</p>"},{"location":"blog/real-time-scatter3d-embedded-in-qwidget-based-application/#result","title":"Result","text":"<p>Now, you can see the graph in a scroll area, with a real-time plot moving randomly in the graph. Also, you can manipulate the purple plots with 2 buttons above the graph.</p> <p></p> <p>Structure of this application.</p> <p></p>"},{"location":"blog/real-time-scatter3d-embedded-in-qwidget-based-application/#references","title":"References","text":"<ul> <li> <p>Qt Forum - Copying C++ Object Data to QML Object (Example: Q3DScatter to Scatter3D)</p> </li> <li> <p>Qt Documentation - Qt Quick Examples - Embedded in Widgets</p> </li> <li> <p>Interacting with QML Objects from C++ | Qt QML 6.7.1</p> </li> <li> <p>QQuickWidget - QQuickView Comparison Example | Qt Quick 6.7.1</p> </li> <li> <p>Qt Quick Examples - Embedded in Widgets | Qt Quick 6.7.1</p> </li> </ul>"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/","title":"Comprehensive Guide to MyoChallenge 2024","text":"<p>This blog post is designed to help you quickly understand and participate in MyoChallenge 2024. It lists all the resources you can refer to at each stage of the challenge.</p> <p>The table of contents to the right explains all the stages you'll go through to participate in MyoChallenge 2024, starting from scratch. Feel free to skip any sections you're already familiar with.</p> <p>Let\u2019s get started!! </p> <p></p>"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#understand-myosuite","title":"Understand MyoSuite","text":"<p>MyoSuite is the foundation of MyoChallenge. It's a platform with a collection of environments/tasks to be solved by musculoskeletal models to understand human dexterity and agility. This is an open-source software (OSS) project, with contributions and participation from researchers, developers, and enthusiasts from across the globe.</p> <p>You can learn basic information about it from these places to get started:</p> <p> <ul> <li> MyoSuite Official Website</li> <li> GitHub - myosuite</li> <li> Welcome to MyoSuite\u2019s documentation!</li> <li> MyoSuite - overview &amp; tutorial (ICRA 2024)</li> </ul> <p></p>"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#do-you-know-gym","title":"Do you know <code>gym</code>?","text":"<p>MyoSuite is basically a superset of <code>gymnasium</code> (former <code>gym</code> maintained by OpenAI) with musculoskeletal extensions, and it generally adheres to its usage.</p> <p>If you're not familiar with the <code>gymnasium</code>, it is highly recommended to get familiar with it first.  </p> <p> <ul> <li> Gymnasium Documentation</li> <li> Tutorials - Gym Documentation</li> </ul> <p></p>"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#learn-usage-with-tutorial","title":"Learn Usage with Tutorial","text":"<p>To get hands-on with MyoSuite, follow the tutorials provided in the official documentation. These tutorials cover various aspects of using MyoSuite, from setting up the environment to running simulations. Especially, it\u2019s better to check the section \u201cUsing Reinforcement Learning\u201d because that is essentially the same process you\u2019ll follow to check you own policy.</p> <p> <ul> <li> Tutorials</li> <li> MyoSuite v2 Tutorial 1 (May 2024) </li> </ul> <p></p>"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#understand-myochallenge-2024","title":"Understand MyoChallenge 2024","text":"<p>MyoChallenge 2024 is a competition held as part of NeurIPS 2024, focusing on advancing physiological dexterity and agility in bionic humans.</p>"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#tasks","title":"Tasks","text":"<p>MyoChallenge 2024 has two types of tasks:</p> <p></p> <p>Manipulation</p> <p>Transfer objects between two points, involving a hand-off between a biological arm (myoArm) and a prosthetic arm (MPL; Modular Prosthetic Limb). Object properties and environmental factors are randomized for each attempt to thoroughly evaluate model performance.</p> <p></p> <p></p> <p>Locomotion</p> <p>Navigate varied terrains using a biological leg (myoLeg) and a prosthetic leg (OSL; Open Source Leg). Challenge difficulty is randomized, potentially altering terrain types and introducing obstacles to test adaptability.</p> <p></p> <p>Both tasks aim to develop strategies to enable both the biological limbs simulated by MyoSim and the prosthetic limbs to work together successfully and achieve objectives.</p> <p>For more detailed descriptions, refer to the official documentation of MyoChallenge 2024.</p> <p> <ul> <li> MyoSuite - MyoChallenge-2024</li> <li> MyoChallenge-2024 Documentations</li> <li> NeurIPS - Competition Track 2024</li> <li> MyoSimposium 2023</li> <li> Medium - MyoSuite Team</li> </ul> <p></p>"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#timeline","title":"Timeline","text":"Time Stage Detail Jul - Aug 2024 Open Stage Release of the models, tasks, and environments. Sep -  Nov 17, 2024 Scored Stage Submissions open on the scoring platform. Dec 10, 2024 Results Announcement @NeurIPS 24 Dec 14, 2024 Workshop @NeurIPS 24"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#prepare-accounts-and-resources","title":"Prepare Accounts and Resources","text":""},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#evalai","title":"EvalAI","text":"<p>MyoChallenge uses EvalAI to manage the competition (e.g. model submission, ranking scores). You need to create an account and get credentials to submit your model. The step-by-step tutorial on how you can get credentials on EvalAI is explained below.</p> <p>Before the Scored stage (Sep-Oct), you can refer to the challenge page for MyoChallenge 2023 to understand how the challenge will be handled on this platform.</p> <p> <ul> <li> Sign up on EvalAI</li> <li> EvalAI - MyoChallenge2024</li> <li> EvalAI - MyoChallenge2023</li> </ul> <p></p>"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#development-environment","title":"Development Environment","text":"<p>MyoSuite only requires a Python environment, but it would be advantageous to prepare a computer or cloud environment with GPU resources to accelerate the training process of the policy. A free GPU resource with limited usage is available on Google Colab.</p> <p>Google Cloud Credits Available!</p> <p> <p>We are pleased to announce that Google Cloud Platform is sponsoring MyoChallenge '24! Participants are now eligible to receive Google Cloud credits, providing access to advanced computational resources on Google Colaboratory.</p> <ul> <li> Apply for Google Cloud Credits</li> </ul> <p></p>"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#formjoin-a-team","title":"Form/Join a Team","text":"<p>Participating in MyoChallenge 2024 can be done individually or as part of a team. Consider the strategy of your participation based on your skills, resources, and goals.</p> <p>To form or join a team:</p> <ol> <li> <p>Review the team formation rule:</p> <p>Team formation / Eligibility:\u00a0There is no limit to the team size and the affiliation: participants from both academic and industrial institutions are welcome. Each participant will be allowed to join only one team during registration and this cannot be changed during the competition. [ref]</p> </li> <li> <p>Network with potential teammates through the official MyoChallenge forums, MyoSuite community, social media, etc</p> </li> <li>Agree on team structure, roles, and communication methods</li> <li>Register your team on EvalAI</li> </ol>"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#develop-your-solution","title":"Develop Your Solution","text":"<p>This is where your skill comes into play! </p>"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#start-with-baseline","title":"Start with Baseline","text":"<p>Before starting the development process, it is recommended to run the pre-trained baselines provided by MyoSuite to assess its performance. Additionally, this can be used as a foundation for your solution.</p> <p>You can visit the following official tutorials on Google Colab to learn how to:</p> <ul> <li>Start a training script that can reproduce the baseline</li> <li>Tune the reward dictionary</li> <li>Access new attributes (e.g., MPL joint angles) and integrate them into your training</li> <li>Submit your policy model</li> </ul> <p> <ul> <li> Tutorial1 - Policy Training with Random Actions &amp; Visualization </li> <li> Tutorial2 - Getting Started with Baselines </li> <li> GitHub - myochallenge_2024eval/tutorials/run_the_baselines.md</li> </ul> <p></p> <p>Tutorials for MyoChallenge 2023 can also be your help.</p> <p> <ul> <li> GitHub - myochallenge_2023eval</li> <li> GitHub - myochallenge_2023eval/tutorials/run_the_baselines.md</li> </ul> <p></p>"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#enhance-the-model","title":"Enhance the Model","text":"<p>Once you've developed your initial model, the next step is to enhance it to improve performance on the tasks. This process may involve experimenting with different RL algorithms, fine-tuning hyperparameters, engineering new features, shaping the reward function, analyzing and visualizing the results, and etc. These techniques can help optimize your model's performance and provide insights into areas for further improvement.</p> <p>The development process is iterative. Continuously evaluate your model's performance, identify areas for improvement, and refine your approach. Regular evaluation is essential to understand how well your enhancements are working and whether further adjustments are needed.</p> <p>You may want to take a look at the strategies of previous MyoChallenge winning teams.</p> <p> <ul> <li> MyoSymposium 2023 - MyoChallenge '23 Winners' Presentation</li> <li> GitHub - myochallenge-lattice</li> </ul> <p></p>"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#submit-your-solution","title":"Submit Your Solution","text":"<p>After training your model, you can submit it by following the steps below. You can achieve this via GitHub Actions or manually by building and uploading a Docker container. The EvalAI credential info is required for both options.</p> GitHub ActionsDIY Submission <ol> <li>Obtain your Auth token on EvalAI</li> <li>Clone the MyoChallenge submission repository: myochallenge_2024eval</li> <li>Register your token as a secret in the repo</li> <li>Edit the existing agent file to use your policy</li> <li>Test your agent</li> <li>Trigger the action</li> </ol> <ol> <li>Install Docker</li> <li>Obtain your Auth token on EvalAI</li> <li>Clone the MyoChallenge submission repository: myochallenge_2024eval</li> <li>Customize Agent Script</li> <li>Build a docker container with the agent</li> <li>Upload the container to the EvalAI docker registry</li> </ol> <p>The Colab below also provides detailed step-by-step instructions for the entire submission process of MyoChallenge 2023.</p> <p> <ul> <li> MyoChallenge '24 Submission Tutorial </li> <li> MyoChallenge '23 Submission Tutorial </li> <li> GitHub - myochallenge_2024eval</li> <li> GitHub - myochallenge_2023eval</li> </ul> <p></p>"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#tips-for-success","title":"Tips for Success","text":""},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#leverage-community-resources","title":"Leverage Community Resources","text":"<p>MyoSuite has an active community of researchers and developers worldwide. Engage with the community to exchange ideas, seek advice, and stay updated on the latest developments. When you have any questions about MyoSuite/MyoChallenge, you can ask the community on both GitHub Issues and GitHub Discussions. </p> <p>Should you encounter any issues or bugs that have not yet been raised, please do not hesitate to submit them as an issue on the GitHub repository. This will help us resolve the problem quickly!</p> <p> <ul> <li> GitHub Issues - myosuite</li> <li> MyoChallenge/MyoSuite Q&amp;A</li> <li> MyoSuite Advocacy Team</li> </ul> <p></p> <p>You can join the MyoSuite community on our official Slack channel from here: </p>"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#collaborate-and-learn","title":"Collaborate and Learn","text":"<p>Participate in webinars or Q&amp;A sessions organized by the MyoChallenge team to deepen your understanding. Connecting with others and forming a team can also enhance your experience and foster collaboration.</p> <p> <ul> <li> MyoSeminar - List of Upcoming/Past Events</li> <li> MyoChallenge '24 Workshop and Q&amp;A Recording</li> <li> MyoSuite Talk @JHU (51 mins)</li> <li> MyoChallenge '24 Talk @JHU (7 mins)</li> </ul> <p></p>"},{"location":"blog/comprehensive-guide-to-myochallenge-2024/#stay-updated","title":"Stay Updated","text":"<p>Please check the MyoChallenge 2024 website regularly for announcements and updates. We also encourage you to follow Myosuite on social media for the latest updates and tips. Staying informed will ensure you don't miss any important notifications.</p> <p> <ul> <li>Follow us on X </li> <li>Give a Star on GitHub </li> </ul> <p></p> <p></p> <p></p> <p>MyoChallenge 2024 is your opportunity to learn and innovate. We encourage you to leverage the resources provided, engage with the community, and enjoy the learning process. Your unique perspective is valuable, and we're excited to see what you'll achieve!</p> <p>Best of luck in your MyoChallenge journey </p> <p></p>"},{"location":"education/","title":"Education &amp; Research     Master of Informatics and Engineering      Bachelor of Informatics and Engineering      Bachelor of Mechanical Engineering","text":"University of Electro-Communications \u00b7 Department of Mechanical and Intelligent Systems Engineering \u00b7 2022-2025  <p> Achievements </p> <p>Our lab successfully commercialized a cyborg prosthetic hand with five independently controlled fingers and an adaptive learning function approved by the Ministry of Health, Labor and Welfare. This is the 2nd electric prosthetic hand covered by insurance in Japan, approved by the goverment, and made in Japan.</p> <p> Research </p> <p>Concentrations:</p> <p>\u00a0\u00a0\u00a0\u00a0 Myoelectric Prosthesis Control, Neuroscience, Biomedical Engineering, Machine Learning, Software Development</p> <p>Thesis:</p> <p>\u00a0\u00a0\u00a0\u00a0 Latent Muscle Manifold Feedback for Pattern Recognition Control of Myoelectric Prosthetic Hands</p> <p>Publications:</p> <ol> <li> <p>T. Tsujimoto et al., \u201cMyoChallenge Manipulation Track: Deep RL for Musculoskeletal and Prosthesis Co-Manipulation,\u201d poster presented at the MyoChallenge Workshop, 38th Annual Conference on Neural Information Processing Systems (NeurIPS2024), Vancouver Convention Center, Vancouver, BC, Canada, Dec. 10\u201315, 2024.</p> </li> <li> <p>T. Tsujimoto et al., \u201cA Training Method with 3D Feature Space Visualization for Pattern Recognition Controlled Myoelectric Prosthetic Hands,\u201d in 12th International Conference on Robot Intelligence Technology and Applications (RiTA2024), Ulsan, Korea, Dec. 4\u20137, 2024.</p> </li> <li> <p>T. Tsujimoto et al., \u201cLatent Feature Space Feedback System for Pattern Recognition Myoelectric Prosthesis Manipulation Training,\u201d poster presented at 6th Annual Conference of the Japanese Society for Regenerative Medicine and Rehabilitation (JSRMR2024), Saitama University, Saitama, Japan, Oct. 26, 2024.</p> </li> <li> <p>T. Tsujimoto et al., \u201cDevelopment of a Training Method for Pattern Identification Control Type Myoelectric Prosthetic Hands Using Feature Space Display Functionality,\u201d in 41st Annual Conference of the Robotics Society of Japan (RSJ2023), Sendai International Center, Sendai, Japan, Sept. 11\u201314, 2023. [link]</p> </li> <li> <p>Y. Kuroda et al., \u201cDevelopment and Clinical Evaluation of a Five-Fingered Myoelectric Prosthetic Hand with Pattern Recognition,\u201d in 2022 IEEE 4th Global Conference on Life Sciences and Technologies (LifeTech2022), Osaka, Japan, 2022, pp. 235\u2013236, doi: 10.1109/LifeTech53646.2022.9754889. [link]</p> </li> </ol> <p>Patents:</p> <ol> <li> <p>H. Yokoi, K. Sakai, Y. Kuroda, K. Yabuki, Y. Yamanoi, and T. Tsujimoto, \"Processing Program, Processing Device, and Robotic Operation System,\" Patent Application No. PCT/JP2024/032987, applied on Sep. 13, 2024, National University Corporation The University of Electro-Communications.</p> </li> <li> <p>H. Yokoi, T. Tsujimoto, Y. Kuroda, K. Yabuki, and Y. Yamanoi, \"Processing Program and Processing Device,\" Patent Application No. 2023-206349, applied on Dec. 6, 2023, National University Corporation The University of Electro-Communications.</p> </li> </ol> Skills <p>              C ,       C++ ,          Python ,              Qt ,           Linux ,           LaTeX ,      SourceTree ,       Bitbucket ,     Google Colab ,         Neural Network ,         Machine Learning ,    Biosignal Processing</p> <p></p> <p></p>      University of Electro-Communications \u00b7 Department of Advanced Robotics Program \u00b7 2020-2022  <p> Awards </p> <p>Elected Valedictorian at graduation and received the Meguro Association Award for the highest academic achievement amongst 686 students.</p> <p> Research </p> <p>I developed and proposed a novel approach to myoelectric prosthetic hand training by introducing real-time feature space visualization. By applying principal component analysis (PCA) to convert complex electromyography (EMG) data into an intuitive two-dimensional format, I enhanced the training experience for users, particularly novices. The core of my project involved developing an Android app for real-time, accessible feedback, leading to improved user proficiency and performance.</p> Skills <p>                      C ,               C++ ,                  Python ,     MATLAB ,                      Qt ,                   Linux ,                   LaTeX ,              SourceTree ,               Bitbucket ,             Google Colab ,                 Neural Network ,                 Machine Learning ,            Biosignal Processing</p> <p></p> <p></p>      Shizuoka University \u00b7 Department of Mechanical Engineering \u00b7 2018-2020  Skills <p>                      C ,                 Arduino ,                  Python ,    Weldering ,        Soldering ,            Signal Processing ,</p> <p></p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/dev/","title":"Dev","text":""},{"location":"blog/category/myosuite/","title":"MyoSuite","text":""},{"location":"blog/category/qt/","title":"Qt","text":""},{"location":"blog/category/essay/","title":"Essay","text":""},{"location":"blog/category/language/","title":"Language","text":""}]}